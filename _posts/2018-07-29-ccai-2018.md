---
layout: post
title:  CCAI 2018参后感
categories: [academic-conference]
---



[2018中国人工智能大会](http://ccai2018.caai.cn/)于2018年7月28，29日在深圳举行。感谢组织提供的机会，笔者有幸参加了这次会议，感觉收获满满。本博文简要记录笔者觉得很有帮助的内容，可以作为今后工作的参考。以下观点仅为笔者个人持有，与笔者所在团队和公司无关，特此声明！



## 会议概要

本次会议持续2天，日程安排非常满，具体安排可以参考[官方日程安排](http://ccai2018.caai.cn/#richeng)。有些议题是并行展开，笔者觉得对工作有帮组的内容如下，

* AI面临的挑战和迁移学习带来的机遇，嘉宾：杨强
* 大规模数据分析及AI助力业务职能转型，嘉宾：戴金权
* 基于海量文本数据的结构化知识抽取：DM，ML和NLP的融合技术，嘉宾：韩家炜，《数据挖掘：概率和技术》作者。
* 深度深林初探，嘉宾：周志华，“机器学习西瓜书”作者。



其他议题，有些是由于笔者阅历和知识储备有限，理解不了，所以不作评价。有些是感觉广告嫌疑太多，而且极度不符合现实，所以也不想进行过多讨论。下面分别对上面四个分享作简要记录，以及个人的一些思考。





## 分享1：AI面临的挑战和迁移学习带来的机遇

数据隐私很重要，欧盟GDPR法案非常严格，如果以后中国也执行类似法案，我们的日常工作可能会遇到同样麻烦。所以，如何在保护隐私的情况下，仍然可以进行机器学习先关的算法研究和应用呢？杨强教授给出的方法是[联邦迁移学习](http://tech.163.com/18/0728/11/DNQ2618G00098IEO.html)，这种思路希望建立起机器学习的企业生态，各个企业自有数据不出本地，模型效果不变，在不违规的情况下建立一个虚拟模型。杨强教授表示，利用联邦迁移学习加密技术，协同建模，学习模型过程不交换用户数，不侵犯隐私。 

联邦迁移学习长远看来，非常值得借鉴。但是，短期内更加值得借鉴的是[迁移学习](https://www.cse.ust.hk/~qyang/Docs/2009/tkde_transfer_learning.pdf)本身，比如解决游戏道具推荐冷启动问题。游戏中同类型的游戏道具非常类似，比如MMO类游戏，道具作用无非就是角色强化，外观装饰，功能等，此方法应该有很大的应用空间。后面先阅读迁移学习的survey，然后结合业务场景作更深入的研究。



## 分享2：大规模数据分析及AI助力业务职能转型

Intel开源了基于Spark的深度学习库，[BigDL](https://github.com/intel-analytics/BigDL)以及[Analytics Zoo](https://github.com/intel-analytics/analytics-zoo)。虽然分享中有广告的嫌疑，表示BigDL只有结合了Intel的CPU，才能发挥最大性能。不过不管怎样，基于Spark的深度学习框架是我们目前热切期盼的工具，是否可以无缝集成到现有工作流中，值得尝试。



## 分享3：基于海量文本数据的结构化知识抽取：DM，ML和NLP的融合技术 

虽然深度学习在NLP中春风得意，但是其实传统ML技术仍然可以解决很多NLP问题，数据挖掘鼻祖韩教授在分享了怎么用传统机器学习，数据挖掘以及NLP技术在文本中挖掘知识，以及应用效果。主要涉及的内容有，

* 挖掘文本结构，包括文本Cube和文本网络；
* 挖掘短语Phrase；
* 挖掘实体关系，笔者认为有点类似知识图谱。



笔者有幸与业界泰斗[韩老师](https://en.wikipedia.org/wiki/Jiawei_Han)共进午餐，韩老师非常健谈，知识渊博，颇受启发。最后得到了韩老师亲笔签名的教材，业余时间一定要抽空拜读韩老师的大作 。

![](/img/ccai_2018/dinner_prof_han.jpg)





## 分享4：深度森林初探 

周志华老师与其所在团队于2017年发表文章[Deep Forest:Towards an Alternative to Deep Neural Networks](https://www.ijcai.org/proceedings/2017/0497.pdf)。至此，深度学习家庭中出现了另外一位成员。截止到其出现之前，深度神经网络一直是深度学习的代名词，而深度神经网络的核心元素可总结为

* 逐层加工
* 数据内部变化，
* 复杂度足够高

深度神经网络虽然在部分NLP问题，以及图像识别问题上是杀手级应用，但是仍存在缺陷，

* 激活函数必须可以微，必须使用BP及其变种算法求解；
* 复杂度在网络架构固定后，不可改变，导致过高的复杂度在简单问题下存在浪费；
* 核心计算非常依赖硬件GPU。

周老师认为，深度学习不应该受限于深度神经网络这些局限，而深度森林正是摆脱这种束缚的一种尝试。不过，周老师最后也提醒大家，深度森林目前还在研究早期，当年深度神经网络从提出到杀手级应用也经过了大概20年的时间，所以短期内不太可能有很明显的效果。

最后分享一个八卦。在提问环节，有个同学怒怼周老师，质疑深度森林的价值，提问：“深度森林的paper目前有被顶级会议收入吗？”。周老师冷静作答：“一项技术的重要性，不应该用是否被顶级会议收入作为衡量指标。时间会证明其价值。”答毕，全场掌声雷动。



## 写在最后

作为数据挖掘从业人员，笔者第一次参加这类学术会议，眼界得到了开阔。后面需要在工作中，有意识的借鉴和应用这次会议学到的内容，作为参加这次会议的回报。







